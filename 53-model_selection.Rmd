## Supplement 2: Model Selection

```{r}
library(performance)
```


```{r}
tar_load(suid)
tar_load(nb_model_suid_count_per_tract)
```

This supplement lays out how we selected the final model for [retrodicting](https://wikidiff.com/predict/retrodict) the number of SUID cases per census tract (variable `suid_count`).

Our base model included the variable `pop_under_five` (population of all children under age 5 years per census tract), which served as a quasi-exposure variable to account for variation in the number of people susceptible to an SUID incident.

### Choosing Model Type

We explored the general family of models that expect an outcome to be distributed as a [count](https://thomaselove.github.io/432-notes/modeling-a-count-outcome-in-ohio-smart.html#a-tobit-censored-regression-model). These distribution types included Poisson, Negative Binomial, and their zero-inflated variants.

#### Overdispersion

Poisson would have been the simplest model choice, but failed on a check for [overdispersion](https://stats.stackexchange.com/questions/554622/meaning-of-overdispersion-in-statistics) when using the base model.

```{r}
glm(
    formula = suid_count ~ public_insurance, 
    family = poisson(), data = suid
) |> 
    check_overdispersion()
```

Using a negative binomial model resolved the overdispersion:

```{r}
MASS::glm.nb(
    suid_count ~ public_insurance,
    data = suid
) |> 
    check_overdispersion()
```

#### Zero-inflation

If a negative binomial model is zero-inflated, one can use a variant of the model to correct for that, but this was not an issue in our context:

```{r}
MASS::glm.nb(
    suid_count ~ public_insurance,
    data = suid
) |> 
    check_zeroinflation()
```

Therefore we settled on a negative binomial model type as our final choice.

### Identifying Retrodictor Candidates

To select retrodictor variable candidates, we sought balance between using variables that were highly correlated with `suid_count` and not using variables too highly correlated with each other (see [multicollinearity](https://www.statology.org/multicollinearity-regression/)).

We started by generating a correlation dataframe and filtering for variables that had at least a weak association (> 0.20) with `suid_count`:

```{r}
suid_correlations <-
    suid |>
    as_tibble() |> 
    select(
        -fips, 
        -geometry, 
        -suid_present,
        -suid_count_factor,
        -approx_suid_incidence
    ) |>
    relocate(suid_count) |> 
    corrr::correlate() |>
    filter(abs(suid_count) > 0.20) |>
    arrange(desc(abs(suid_count))) 

suid_correlations[,1:2]
```

`publicinsurance`, the percentage of residents in each census tract on public insurance, had the strongest correlation with `suid_count`. 

Here is the relationship visualized:

```{r}
as_tibble(suid) |> 
    correlation::cor_test("suid_count", "public_insurance") |> 
    plot()
```

Next we next selected for variables that had some degree of correlation with `suid_count`, but had no more than weak correlation (< 0.5) with `publicinsurance`:

```{r}
suid_correlations |>
    filter(abs(suid_count) > 0.20) |>
    filter(abs(public_insurance) < 0.5) |> 
    select(term, suid_count, public_insurance)
```

This just left `count_opioid_death`, the count of opioid-related deaths in each census tract.

```{r}
as_tibble(suid) |> 
    correlation::cor_test("suid_count", "count_opioid_death") |> 
    plot()
```

There was a large outlier of many opioid deaths in a tract with no SUID deaths, but otherwise, there seemed to be a strong positive trend.

To fill out our candidates, we selected the next four variables that correlated most with `suid_count`, without being redundant (e.g. not selecting `black` when `white` was already in the list):

- `white` = the percentage of residents in each census tract identifying their race as White 
- `svi_household_composition_disability` = percentile ranking for each census tract on the [Social Vulnerability Index](https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation_01192022_1.pdf) for Household Composition & Disability, which is a mash-up of information about households that include people older than 65, people younger than 17, people with disabilities, and/or single-parents
- `income_gt_75` = the percentage of residents in each census tract whose income were greater than the national 75th percentile
- `married_females` = the percentage of female residents who were married in each census tract

Here are the summary statistics and visualized distributions of our candidate retrodictors:

```{r}
as_tibble(suid) |>
    select(
        pop_under_five,
        public_insurance,
        count_opioid_death,
        white,
        svi_household_composition_disability,
        income_gt_75,
        married_females
    ) %T>%
    DataExplorer::plot_histogram() |>
    summary()
```


### Selecting retrodictor Variables

Next, we compared performance of a nested sequence of retrodictor candidates:

```{r}
list(
    one_beta = suid_count ~ public_insurance,
    two_betas = suid_count ~ public_insurance + count_opioid_death,
    three_betas = suid_count ~ public_insurance + count_opioid_death + white,
    four_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability,
    five_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability + income_gt_75,
    six_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability + income_gt_75 + married_females
) |> 
    purrr::map(~MASS::glm.nb(.x, data = suid)) |> 
    compare_performance(rank = TRUE) |>  
    print_html() 
```

We used the `compare_performance()` function's [ranking algorithm](https://easystats.github.io/performance/reference/compare_performance.html#ranking-models), which chose the base model plus 3 retrodictors as performing the best. 

### Selecting Interaction Terms

Then we used the `select_parameters()` function's [heuristic algorithm](https://easystats.github.io/parameters/reference/select_parameters.html) to check if any interaction terms were worth including in the model.

```{r}
MASS::glm.nb(
    formula = suid_count ~ (public_insurance + count_opioid_death + white)^2,
    data = tar_read(suid)
) |> 
parameters::select_parameters() |> 
parameters::parameters() |> 
parameters::print_html()
```

The interaction between `ccount_opioid_death` and `white` was proposed by the algorithm and statistically significant, so we included it as the only interaction term.

```{r}
list(
    no_interaction = suid_count ~ public_insurance + count_opioid_death + white,
    one_interaction = suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white,
    two_interactions = suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white + public_insurance:white
) |> 
    purrr::map(~MASS::glm.nb(.x, data = suid)) |> 
    compare_performance(rank = TRUE) |>  
    print_html() 
```

```{r}
library(brms)
suid$obs <- 1:nrow(suid)

fit <- brm(suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white, data = suid, family = poisson())
```

```{r}
summary(fit)
```

```{r}
plot_rootogram(fit)
```


### Visually Checking Model Fit

We plotted a [rootogram](https://www.r-bloggers.com/2016/06/rootograms/) of the final model to visualize its goodness of fit:

```{r}
source("R/plot_rootogram.R")

plot_rootogram(MASS::glm.nb(suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white, data = suid))
```

Overall, the model did an excellent job of retrodicting whether a tract had zero incidents, somewhat over-retrodicted tracts with 1 incident, and did a poor job of retrodicting tracts with 2 or more incidents.

We saw this lopsided issue with performance when visualizing residuals too. Here is the distribution of each deviance value from the fitted data:

```{r}
insight::get_residuals(nb_model_suid_count_per_tract) |> 
    DataExplorer::plot_histogram()
```

And here is a QQ Plot showing a violation of the normality assumption for residuals:

```{r}
check_model(nb_model_suid_count_per_tract, check = "qq")
```

All this to say that although our final model performed well compared to a simpler Poisson regression (and better still than a linear regression), it was still far from perfect.

