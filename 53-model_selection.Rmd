## Supplement 2: Model Selection

```{r}
library(tidyverse)
library(sf)
library(targets)
library(correlation)
library(performance)

tar_load(suid_training_data)
```

This supplement lays out how we selected the final model for predicting the number of SUID cases per census tract during 2015-2019 (variable `suid_count`) from variables in the 2014 vintage of the [Social Vulnerability Index (SVI) dataset](https://www.atsdr.cdc.gov/placeandhealth/svi/).

<!-- Our base model included the variable `pop_under_five` (population of all children under age 5 years per census tract), which served as a quasi-exposure variable to account for variation in the number of people susceptible to an SUID incident. -->

### Choosing Model Type

We explored the general family of models that expect an outcome to be distributed as a [count](https://thomaselove.github.io/432-notes/modeling-a-count-outcome-in-ohio-smart.html#a-tobit-censored-regression-model). These distribution types include Poisson, Negative Binomial, and their zero-inflated variants.

#### Overdispersion

Poisson would have been the simplest model choice, but is unable to account for [overdispersion](https://stats.stackexchange.com/questions/554622/meaning-of-overdispersion-in-statistics), which was present in our data:

```{r}
glm(
    formula = suid_count ~ 1, 
    family = poisson(), 
    data = suid_train
) |> check_overdispersion()
```

Using a negative binomial model resolved the overdispersion:

```{r}
MASS::glm.nb(
    suid_count ~ 1,
    data = suid_train
) |> check_overdispersion()
```

#### Zero-inflation

If a negative binomial model is zero-inflated, one can use a variant of the model to correct for that, but this was not an issue in our context:

```{r}
MASS::glm.nb(
    suid_count ~ 1,
    data = suid_train
) |> check_zeroinflation()
```

Therefore we settled on a negative binomial model type as our final choice.

### Identifying Predictor Candidates

We generated a dataframe containing correlation coefficients for each variable to `suid_count`:

```{r}
suid_correlations <-
    suid_train |>
    as_tibble() |> 
    select(
        suid_count,
        matches("^ep_")
    ) |>
    corrr::correlate() |> 
    arrange(desc(abs(suid_count))) 

suid_correlations[1:10,1:2]
```

Variables in the SVI dataset are broken up into four thematic categories. From each theme, we chose the most correlated variable for inclusion in the model: 

- Theme 1 (Socioeconomic): `ep_unemp` - Estimated percentage of unemployed civilians (age 16+)
- Theme 2 (Household Composition/Disability): `ep_sngpnt` - Estimated percentage of single parent households with children under 18, 2010-2014 ACS
- Theme 3 (Minority Status/Language): `ep_minrty` - Estimated percentage of minority persons (all persons except white, non-Hispanic), 2010-2014 ACS
- Theme 4 (Housing Type/Transportation): `ep_noveh` - Estimated percentage of households with no vehicle available 
 
Here are the relationships visualized:

```{r}
suid_train |> 
    ggplot(aes(x = ep_unemp, y = suid_count)) +
    geom_point() +
    labs(x = "Unemployed (%)", y = "SUID cases") +
    theme_bw()
```

```{r}
suid_train |> 
    ggplot(aes(x = ep_sngpnt, y = suid_count)) +
    geom_point() +
    labs(x = "Single-Parent Households (%)", y = "SUID cases") +
    theme_bw()
```

```{r}
suid_train |> 
    ggplot(aes(x = ep_minrty, y = suid_count)) +
    geom_point() +
    labs(x = "Minority Persons (%)", y = "SUID cases") +
    theme_bw()
```

```{r}
suid_train |> 
    ggplot(aes(x = ep_noveh, y = suid_count)) +
    geom_point() +
    labs(x = "Households Without a Vehicle (%)", y = "SUID cases") +
    theme_bw() 
```

```{r}
suid_negbinomial_full <- 
    brm(
        suid_count ~ ep_unemp + ep_minrty + ep_sngpnt + ep_noveh, 
        data = suid_train,
        family = negbinomial()
    )
    

suid_negbinomial_intercept <- 
    brm(
        suid_count ~ 1, 
        data = suid_train,
        family = negbinomial()
    )

suid_gaussian_full <- 
    brm(
        suid_count ~ ep_unemp + ep_minrty + ep_sngpnt + ep_noveh, 
        data = suid_train,
        family = gaussian()
    )

tar_load(logistic_full_model)
tar_load(logistic_intercept_model)

logistic_3_var <-
    glm(
            suid_present ~ ep_unemp + ep_minrty + ep_sngpnt,
            data = suid_training_data,
            family = binomial(link = "logit") 
        )
```

```{r}
compare_performance(logistic_3_var, logistic_full_model, logistic_intercept_model, metrics = "common")
```

```{r}
performance_accuracy(logistic_3_var)
```


```{r}
performance_accuracy(logistic_full_model)
```

```{r}
mcmc_trace(suid_negbinomial_full)
```

```{r}
mcmc_dens_overlay(suid_negbinomial_full)
```

```{r}
mcmc_acf(suid_negbinomial_full)
```

```{r}
pp_check(suid_negbinomial_full, ndraws = 50) +
    scale_y_sqrt() +
    coord_cartesian(xlim = c(0, 8))
```

```{r}
parameters::parameters(suid_negbinomial_full, exponentiate = TRUE)
```

```{r}
insight::get_predicted(suid_negbinomial_full)
```


```{r}
performance(the_model)

parameters::select_parameters(the_model)

compare_performance(suid_negbinomial_full, suid_negbinomial_1_summary)

source("R/plot_rootogram.R")

plot_rootogram(suid_gaussian_full)

suid_prediction |> 
    correlation::cor_test("suid_count", "ep_unemp") |> 
    plot()

summary(four_params)

performance(four_params)

parameters(four_params, exponentiate = TRUE)

pp_check(suid_negbinomial_intercept)

compare_performance(suid_gaussian_full, suid_negbinomial_intercept, suid_negbinomial_full)
```

```{r}
suid_train <-
    suid_train |> 
    mutate(
        .prediction = posterior_predict(suid_negbinomial_full),
        .dev_residual = 
            case_when(
                suid_count == 0 ~ suid_count - .prediction,
                TRUE ~ suid_count*log(suid_count/.prediction) - (suid_count - .prediction)
            )
    )

suid_prediction |> 
    ggplot(aes(x = .dev_residual)) +
    geom_histogram() 
```

```{r}
library(tmap)

tmap_mode("view")

tm_shape(suid_prediction) + tm_polygons(col = ".dev_residual", alpha = 0.5)
```


To fill out our candidates, we selected the next four variables that correlated most with `suid_count`, without being redundant (e.g. not selecting `black` when `white` was already in the list):

- `white` = the percentage of residents in each census tract identifying their race as White 
- `svi_household_composition_disability` = percentile ranking for each census tract on the [Social Vulnerability Index](https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation_01192022_1.pdf) for Household Composition & Disability, which is a mash-up of information about households that include people older than 65, people younger than 17, people with disabilities, and/or single-parents
- `income_gt_75` = the percentage of residents in each census tract whose income were greater than the national 75th percentile
- `married_females` = the percentage of female residents who were married in each census tract

Here are the summary statistics and visualized distributions of our candidate retrodictors:

```{r}
as_tibble(suid) |>
    select(
        pop_under_five,
        public_insurance,
        count_opioid_death,
        white,
        svi_household_composition_disability,
        income_gt_75,
        married_females
    ) %T>%
    DataExplorer::plot_histogram() |>
    summary()
```


### Selecting retrodictor Variables

Next, we compared performance of a nested sequence of retrodictor candidates:

```{r}
list(
    one_beta = suid_count ~ public_insurance,
    two_betas = suid_count ~ public_insurance + count_opioid_death,
    three_betas = suid_count ~ public_insurance + count_opioid_death + white,
    four_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability,
    five_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability + income_gt_75,
    six_betas = suid_count ~ public_insurance + count_opioid_death + white + svi_household_composition_disability + income_gt_75 + married_females
) |> 
    purrr::map(~MASS::glm.nb(.x, data = suid)) |> 
    compare_performance(rank = TRUE) |>  
    print_html() 
```

We used the `compare_performance()` function's [ranking algorithm](https://easystats.github.io/performance/reference/compare_performance.html#ranking-models), which chose the base model plus 3 retrodictors as performing the best. 

### Selecting Interaction Terms

Then we used the `select_parameters()` function's [heuristic algorithm](https://easystats.github.io/parameters/reference/select_parameters.html) to check if any interaction terms were worth including in the model.

```{r}
MASS::glm.nb(
    formula = suid_count ~ (public_insurance + count_opioid_death + white)^2,
    data = tar_read(suid)
) |> 
parameters::select_parameters() |> 
parameters::parameters() |> 
parameters::print_html()
```

The interaction between `ccount_opioid_death` and `white` was proposed by the algorithm and statistically significant, so we included it as the only interaction term.

```{r}
list(
    no_interaction = suid_count ~ public_insurance + count_opioid_death + white,
    one_interaction = suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white,
    two_interactions = suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white + public_insurance:white
) |> 
    purrr::map(~MASS::glm.nb(.x, data = suid)) |> 
    compare_performance(rank = TRUE) |>  
    print_html() 
```

```{r}
library(brms)
suid$obs <- 1:nrow(suid)

fit <- brm(suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white, data = suid, family = poisson())
```

```{r}
summary(fit)
```

```{r}
plot_rootogram(fit)
```


### Visually Checking Model Fit

We plotted a [rootogram](https://www.r-bloggers.com/2016/06/rootograms/) of the final model to visualize its goodness of fit:

```{r}
source("R/plot_rootogram.R")

plot_rootogram(MASS::glm.nb(suid_count ~ public_insurance + count_opioid_death + white + count_opioid_death:white, data = suid))
```

Overall, the model did an excellent job of retrodicting whether a tract had zero incidents, somewhat over-retrodicted tracts with 1 incident, and did a poor job of retrodicting tracts with 2 or more incidents.

We saw this lopsided issue with performance when visualizing residuals too. Here is the distribution of each deviance value from the fitted data:

```{r}
insight::get_residuals(nb_model_suid_count_per_tract) |> 
    DataExplorer::plot_histogram()
```

And here is a QQ Plot showing a violation of the normality assumption for residuals:

```{r}
check_model(nb_model_suid_count_per_tract, check = "qq")
```

All this to say that although our final model performed well compared to a simpler Poisson regression (and better still than a linear regression), it was still far from perfect.

